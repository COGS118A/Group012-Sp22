{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/COGS118A/Group012-Sp22/blob/main/COGS_118A_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crUlTnX2e_IV"
   },
   "source": [
    "## Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQ8qHZrHZb_P",
    "outputId": "7675b1f0-2166-402a-fe76-94e6122ad50b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNvyP-RtYQRv"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VhAHOcOE9ec",
    "outputId": "ea4d6fb9-4519-4a38-8f22-fc4a445650ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sH79viV-B7Cr",
    "outputId": "f1e06e47-1bd5-442c-f685-5323c01d38c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34madhd\u001b[0m/  \u001b[01;34malcoholism\u001b[0m/  \u001b[01;34manxiety\u001b[0m/  \u001b[01;34mdepression\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls drive/MyDrive/'Courses UCSD'/'SPRING 2022'/'COGS 118A'/'FinalProject'/'reddit_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXG7_KZHCxIo"
   },
   "outputs": [],
   "source": [
    "data_path = 'drive/MyDrive/Courses UCSD/SPRING 2022/COGS 118A/FinalProject/reddit_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTGsjjX1cuTg"
   },
   "source": [
    "## Datasets to Dataframes\n",
    "\n",
    "Import all of the data files for each disorder and merge them into one dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA92WtqiDoVE"
   },
   "source": [
    "ADHD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywfGC68IDBGY"
   },
   "outputs": [],
   "source": [
    "# import data from drive\n",
    "adhd_2018_df = pd.read_csv(data_path + '/adhd/adhd_2018.csv')\n",
    "adhd_2019_df = pd.read_csv(data_path + '/adhd/adhd_2019.csv')\n",
    "adhd_post_df = pd.read_csv(data_path + '/adhd/adhd_post.csv')\n",
    "adhd_pre_df  = pd.read_csv(data_path + '/adhd/adhd_pre.csv')\n",
    "\n",
    "# join all data into one DataFrame\n",
    "adhd_dataset = pd.concat([adhd_2018_df, adhd_2019_df, adhd_post_df, adhd_pre_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeoU9HWGEdZj"
   },
   "source": [
    "ANXIETY DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJUplpheEdv5"
   },
   "outputs": [],
   "source": [
    "# import data from drive\n",
    "anxiety_2018_df = pd.read_csv(data_path + '/anxiety/anxiety_2018.csv')\n",
    "anxiety_2019_df = pd.read_csv(data_path + '/anxiety/anxiety_2019.csv')\n",
    "anxiety_post_df = pd.read_csv(data_path + '/anxiety/anxiety_post.csv')\n",
    "anxiety_pre_df  = pd.read_csv(data_path + '/anxiety/anxiety_pre.csv')\n",
    "\n",
    "# join all data into one DataFrame\n",
    "anxiety_dataset = pd.concat([anxiety_2018_df, anxiety_2019_df, anxiety_post_df, anxiety_pre_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1-Ef8BzEgUw"
   },
   "source": [
    "ALCOHOLISM DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQ_li5i6Eick"
   },
   "outputs": [],
   "source": [
    "# import data from drive\n",
    "alcoholism_2018_df = pd.read_csv(data_path + '/alcoholism/alcoholism_2018.csv')\n",
    "alcoholism_2019_df = pd.read_csv(data_path + '/alcoholism/alcoholism_2019.csv')\n",
    "alcoholism_post_df = pd.read_csv(data_path + '/alcoholism/alcoholism_post.csv')\n",
    "alcoholism_pre_df  = pd.read_csv(data_path + '/alcoholism/alcoholism_pre.csv')\n",
    "\n",
    "# join all data into one DataFrame\n",
    "alcoholism_dataset = pd.concat([alcoholism_2018_df, alcoholism_2019_df, alcoholism_post_df, alcoholism_pre_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3g5OL2MxDz9P"
   },
   "source": [
    "DEPRESSION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7OG2mThWWcx"
   },
   "outputs": [],
   "source": [
    "# import data from drive\n",
    "depression_2018_df = pd.read_csv(data_path + \"/depression/depression_2018.csv\")\n",
    "depression_2019_df = pd.read_csv(data_path + \"/depression/depression_2019.csv\")\n",
    "depression_post_df = pd.read_csv(data_path + \"/depression/depression_post.csv\")\n",
    "depression_pre_df  = pd.read_csv(data_path + \"/depression/depression_pre.csv\")\n",
    "\n",
    "# join all data into one DataFrame\n",
    "depression_dataset = pd.concat([depression_2018_df, depression_2019_df, depression_post_df, depression_pre_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ut4YmW3UpI4b",
    "outputId": "b7a9d80c-0071-4546-ab0d-7d813070f865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adhd 45631\n",
      "anxiety 57671\n",
      "alcoholim 5911\n",
      "depression 117331\n"
     ]
    }
   ],
   "source": [
    "# length of our data\n",
    "print('adhd', len(adhd_dataset))\n",
    "print('anxiety', len(anxiety_dataset))\n",
    "print('alcoholim', len(alcoholism_dataset))\n",
    "print('depression', len(depression_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFt66Wu7euHm"
   },
   "source": [
    "### Example of a post from a dataset\n",
    "\n",
    "Our data consists of reddit posts organized into distinct mental health support groups, no labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5H-iNCHWdSP",
    "outputId": "55bc6502-3bfc-433f-c9d0-42c44f039178"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Concerta not working on the first day?! Update...\n",
       "1    I was doing so well... I was diagnosed back in...\n",
       "1    The First Step of a multi-step task Perhaps yo...\n",
       "1    ADHD &amp; Bipolar Anyone else have Bipolar Di...\n",
       "Name: post, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adhd_post = adhd_dataset.loc[:, \"post\"][1]\n",
    "adhd_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_72ftSnhNB7"
   },
   "source": [
    "### Data Noise Reduction and Format Simplification\n",
    "\n",
    "Let's isolate our noisy dataframes into a more neutral list format, with only the reddit posts as each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQ4NEaMCi0TK"
   },
   "outputs": [],
   "source": [
    "adhd_posts = [i for i in adhd_dataset.loc[:, \"post\"]]\n",
    "anxiety_posts = [i for i in anxiety_dataset.loc[:, \"post\"]]\n",
    "alcoholism_posts = [i for i in alcoholism_dataset.loc[:, \"post\"]]\n",
    "depression_posts = [i for i in depression_dataset.loc[:, \"post\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TQLeBYdDbQ2"
   },
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20pjM6XHgfSo"
   },
   "outputs": [],
   "source": [
    "#functions to remove stopwords from posts\n",
    "\n",
    "def remove_stops(text, stops):\n",
    "  words = text.split()\n",
    "  final = []\n",
    "  for word in words:\n",
    "    if word not in stops:\n",
    "      final.append(word)\n",
    "  final = \" \".join(final)\n",
    "  final = final.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "  final = \"\".join([i for i in final if not i.isdigit()])\n",
    "  while \"  \" in final:\n",
    "    final = final.replace(\"  \", \" \")\n",
    "  return final\n",
    "\n",
    "def clean_docs(docs):\n",
    "  stops = stopwords.words(\"english\")\n",
    "  final = []\n",
    "  final2 = []\n",
    "  for doc in docs:\n",
    "    clean_doc = remove_stops(doc, stops)\n",
    "    final.append(clean_doc)\n",
    "\n",
    "  return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqnI-yn-gpLT"
   },
   "source": [
    "Clean all of our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEop2fqyimUq"
   },
   "outputs": [],
   "source": [
    "cleaned_adhd_docs = clean_docs(adhd_posts)\n",
    "cleaned_anxiety_docs = clean_docs(anxiety_posts)\n",
    "cleaned_alcoholism_docs = clean_docs(alcoholism_posts)\n",
    "cleaned_depression_docs = clean_docs(depression_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szg-fx_-1cFl",
    "outputId": "9911c0db-efa4-4281-c799-27652211a7e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adhd 45631\n",
      "anxiety 57671\n",
      "alcoholism 5911\n",
      "depression 117331\n"
     ]
    }
   ],
   "source": [
    "#check for an data loss\n",
    "\n",
    "print('adhd', len(cleaned_adhd_docs))\n",
    "print('anxiety', len(cleaned_anxiety_docs))\n",
    "print('alcoholism', len(cleaned_alcoholism_docs))\n",
    "print('depression', len(cleaned_depression_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6r3ifzkTDQ-k"
   },
   "source": [
    "## Compare Clean vs Unclean Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvQz6D29i6c8"
   },
   "source": [
    "### From our ADHD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "E2ejYuIhDO7m",
    "outputId": "3ac5dd02-e9db-402d-bf5d-2e9e136219f7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Concerta not working on the first day?! Update!: Thank you all for your insightful and kind responses, if not for them I would have chickened out ONCE AGAIN! I took my meds (Concerta 18mg) at 3:15 and I feel absolutely NOTHING! I know some people say they feel nothing and perhaps there’s a slight change or feeling/“buzz” but I feel ABSOLUTELY NOTHING. I feel exactly the same as I did prior to taking it. I actually just took a nap otherwise I would have updated earlier with this post:)\\n\\nI should mention that prior to taking my meds I took some vitamins including:\\nB Complex\\nVitamin D3\\nZinc\\nOmega 3-6\\n\\nDoes this have any affect on the Concerta? \\nAlso is there any point in continuing or should I simply get a higher dose? \\n\\nAs always, Any advice would be greatly appreciated:) '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adhd_posts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "vuHmDYsvDKoA",
    "outputId": "e935c721-ba03-49bb-a1e7-94029ff031ba"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Concerta working first day Update Thank insightful kind responses I would chickened ONCE AGAIN I took meds Concerta mg I feel absolutely NOTHING I know people say feel nothing perhaps there’s slight change feeling“buzz” I feel ABSOLUTELY NOTHING I feel exactly I prior taking it I actually took nap otherwise I would updated earlier post I mention prior taking meds I took vitamins including B Complex Vitamin D Zinc Omega Does affect Concerta Also point continuing I simply get higher dose As always Any advice would greatly appreciated'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_adhd_docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6QhDzzpi9Hb"
   },
   "source": [
    "### From our Alcoholism Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "WAbSrrsAjEHS",
    "outputId": "4620189d-5297-40af-b278-f095c2489111"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'It’s 1:30am. So glad to be sober. I was at a New Years party tonight.  All the adults got wasted and had a good time.   I did not get wasted and I had a good time. \\n\\n\\nTomorrow I’ll be feeling good.  Not sure my fellow partiers will.  \\n\\n\\nSo glad to be sober. '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alcoholism_posts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "CIFpahYCjB7y",
    "outputId": "d01704eb-ffa5-4329-b46a-f8f4ca239489"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'It’s am So glad sober I New Years party tonight All adults got wasted good time I get wasted I good time Tomorrow I’ll feeling good Not sure fellow partiers will So glad sober'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_alcoholism_docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAYL-1XKa7y7"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03hymgfWe7RN",
    "outputId": "858509bc-ff6e-4d9b-f751-2b459ad6d080"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'al', 'couldn', 'daren', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'itse', 'll', 'mayn', 'mightn', 'mon', 'mustn', 'myse', 'needn', 'oughtn', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "custom_stopwords = 'drive/MyDrive/Courses UCSD/SPRING 2022/COGS 118A/FinalProject/stop_words_english.txt'\n",
    "\n",
    "with open(custom_stopwords, 'r') as f:\n",
    "    more_stop_words = [line.strip() for line in f]\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(more_stop_words)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "                                lowercase=True,\n",
    "                                max_features=300,\n",
    "                                stop_words=my_stop_words)\n",
    "\n",
    "vectors = vectorizer.fit_transform(cleaned_adhd_docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "adhd_tfidf = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpUokjRXfmW_"
   },
   "outputs": [],
   "source": [
    "adhd_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgwv9Na7mZ2w"
   },
   "outputs": [],
   "source": [
    "#visualize only the keywords\n",
    "\n",
    "all_keywords = []\n",
    "\n",
    "for description in denselist:\n",
    "  x = 0\n",
    "  keywords = []\n",
    "  for word in description:\n",
    "    if word > 0:\n",
    "      keywords.append(feature_names[x])\n",
    "    x = x+1\n",
    "  all_keywords.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1jfCwySpIdH"
   },
   "outputs": [],
   "source": [
    "print(all_keywords[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qz_03oEZtHTS"
   },
   "outputs": [],
   "source": [
    "## if we are interested in the n features with the highest TF IDF scores\n",
    "\n",
    "top_n = 300\n",
    "top_n_features = sorted(list(zip(feature_names, \n",
    "                                  vectors.sum(0).getA1())), \n",
    "                              key=lambda x: x[1], reverse=True)[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjkOeGWBuYis"
   },
   "outputs": [],
   "source": [
    "#top_n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvp4gE-muboH"
   },
   "outputs": [],
   "source": [
    "for feature in top_n_features:\n",
    "  if feature[0] == 'suicide':\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QO54Thkvv-v"
   },
   "outputs": [],
   "source": [
    "# Extract the TF-IDF seed words from the 2018 depression dataset\n",
    "my_seed_words = []\n",
    "for feature in top_n_features:\n",
    "  my_seed_words.append(feature[0])\n",
    "print(my_seed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAT66TS0i7Ip"
   },
   "source": [
    "todo: go through all studies and create a comprehensive list of all keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDJu2Jr1T_5h"
   },
   "outputs": [],
   "source": [
    "# TF-IDF seed words from existing study\n",
    "depression_true_seed_words = ['myself', 'really', 'depression', 'hope', 'life', 'forever', 'pain', 'sad', 'live', 'mood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FOguDXyWv85"
   },
   "outputs": [],
   "source": [
    "# Calculate Cosine Similarity between the two seed word lists\n",
    "from collections import Counter\n",
    "\n",
    "# count word occurrences\n",
    "our_vals = Counter(my_seed_words)\n",
    "true_vals = Counter(depression_true_seed_words)\n",
    "\n",
    "# convert to word-vectors\n",
    "words  = list(our_vals.keys() | true_vals.keys())\n",
    "our_vect = [our_vals.get(word, 0) for word in words]        # [0, 0, 1, 1, 2, 1]\n",
    "true_vect = [true_vals.get(word, 0) for word in words]        # [1, 1, 1, 0, 1, 0]\n",
    "\n",
    "# find cosine\n",
    "len_our  = sum(av*av for av in our_vect) ** 0.5             # sqrt(7)\n",
    "len_true  = sum(bv*bv for bv in true_vect) ** 0.5             # sqrt(4)\n",
    "dot    = sum(av*bv for av,bv in zip(our_vect, true_vect))    # 3\n",
    "cosine = dot / (len_our * len_true)                          # 0.5669467\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0v5GdAYXPi8"
   },
   "outputs": [],
   "source": [
    "print(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeFVBtGaXTW0"
   },
   "outputs": [],
   "source": [
    "depression_lexicon = ['depressed', 'tired', 'anxious', 'sleep', 'insomnia', 'sad', 'meaningless','goodbye', 'pointless', 'angry', 'suicidal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBiaeYN7BJXx"
   },
   "source": [
    "# Self-classification with Keras (Masked Language Modeling)\n",
    "\n",
    "The following code creates labels automatically using Masked Language Modeling, with one Neural Network layer for our dataset, since reddit posts unfortunately do not come prelabeled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZLOCpZFLSW7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjoUQ88ajxSx"
   },
   "source": [
    "This is a test dummy dataset only, labels are fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5NP4nwkQpbP"
   },
   "outputs": [],
   "source": [
    "# make the adhd features into a tensor\n",
    "adhd_features = tf.constant(cleaned_adhd_docs)\n",
    "\n",
    "# make the adhd labels into a tensor\n",
    "# these are temporary labels just to make a temporary dataset\n",
    "adhd_labels = tf.constant(np.random.choice([0, 1], size=(len(cleaned_adhd_docs),), p=[1./3, 2./3]))\n",
    "\n",
    "# initialize a tensorflow dataset for text features and labels\n",
    "# wel will use this dataset to extract a lexicon out of all data samples\n",
    "# so that we can train a neural network with it\n",
    "adhd_dataset = tf.data.Dataset.from_tensor_slices((adhd_features, adhd_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpxfhC6gWDDM",
    "outputId": "731f066c-f64b-4a21-cdc6-282dc6d69e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'LethargicDepressed meds First Ill give background medical history I struggled depression years nowIm currently would bouts crying absolute consumption life every week so About months ago I diagnosed ADD started taking mg xr adderall morning noon About montg starting I noticed I longer bouts depression overall happier My mom super anti meds blames everything bad adderall take away randomly Every time I feel withdrawals bouts depression come back I feel like I want lie die Does anyone else feel way me'\n",
      "1\n",
      "b'Concerta working first day Update Thank insightful kind responses I would chickened ONCE AGAIN I took meds Concerta mg I feel absolutely NOTHING I know people say feel nothing perhaps there\\xe2\\x80\\x99s slight change feeling\\xe2\\x80\\x9cbuzz\\xe2\\x80\\x9d I feel ABSOLUTELY NOTHING I feel exactly I prior taking it I actually took nap otherwise I would updated earlier post I mention prior taking meds I took vitamins including B Complex Vitamin D Zinc Omega Does affect Concerta Also point continuing I simply get higher dose As always Any advice would greatly appreciated'\n",
      "0\n",
      "b'Comorbid anxiety ADHDPI Medication Question Hi I recently diagnosed ADHDPi I adult I also problems anxiety Personally I believe anxiety related inattentive symptoms It usually occurs life seems hectic control I much going on My psychiatrist perscribed SSRI start on I understand treat anxiety I worried treating wrong thing I sure usual procedure ADHD anxiety Does anyone else experience ADHD anxiety How treated It normal start SSRI Thank'\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## this just displays the first couple data points and their classification label\n",
    "\n",
    "for text_batch, label_batch in adhd_dataset.take(3):\n",
    "        print(text_batch.numpy())\n",
    "        print(label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv6bEfSbeA7q"
   },
   "source": [
    "### Dataset Prep: Vocabulary and Mask Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTqxXcRyTer8"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhWImm8tdOGH"
   },
   "outputs": [],
   "source": [
    "## data cleaning from capitalization and symbols\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yk3--q4TNRD"
   },
   "outputs": [],
   "source": [
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence lenght.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize vocabulary layer, creates a lexicon to adapt our model with\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "\n",
    "    # use the entire dataset (no labels) and create a useful lexicon out of it:\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    # Insert mask token in vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnX0KleGkH3G"
   },
   "source": [
    "## Text Vectorization with Vocabulary Layer\n",
    "\n",
    "- the layer will build a vocabulary of all string tokens seen in the dataset, sorted by occurance count, with ties broken by sort order of the tokens (high to low). \n",
    "- Will compute the most frequent tokens occurring in the input dataset.\n",
    "- We use this 'vocab' to train our model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcdlIaOpkIOQ"
   },
   "outputs": [],
   "source": [
    "'''ADHD'''\n",
    "# run our cleaned ADHD data through the vocab layer\n",
    "adhd_vectorize_layer = get_vectorize_layer(\n",
    "    cleaned_adhd_docs,\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Get mask token id for masked language model\n",
    "adhd_mask_token_id = adhd_vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sTZAKsqma5M"
   },
   "outputs": [],
   "source": [
    "'''ANXIETY'''\n",
    "# run our cleaned ADHD data through the vocab layer\n",
    "anxiety_vectorize_layer = get_vectorize_layer(\n",
    "    cleaned_anxiety_docs,\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Get mask token id for masked language model\n",
    "anxiety_mask_token_id = anxiety_vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IWmCESImbHz"
   },
   "outputs": [],
   "source": [
    "'''ALCOHOLISM'''\n",
    "# run our cleaned ADHD data through the vocab layer\n",
    "alcoholism_vectorize_layer = get_vectorize_layer(\n",
    "    cleaned_alcoholism_docs,\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Get mask token id for masked language model\n",
    "alcoholism_mask_token_id = alcoholism_vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB4MyEsNmbUr"
   },
   "outputs": [],
   "source": [
    "# NOT ENOUGH RAM ON COLAB\n",
    "\n",
    "# '''DEPRESSION'''\n",
    "# # run our cleaned ADHD data through the vocab layer\n",
    "# depression_vectorize_layer = get_vectorize_layer(\n",
    "#     cleaned_depression_docs,\n",
    "#     config.VOCAB_SIZE,\n",
    "#     config.MAX_LEN,\n",
    "#     special_tokens=[\"[mask]\"],\n",
    "# )\n",
    "\n",
    "# # Get mask token id for masked language model\n",
    "# depression_mask_token_id = depression_vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2gU_kzfVf9i"
   },
   "source": [
    "Encoding and Self-Classification with Masked Language Modeling\n",
    "\n",
    "Code sample from Keras Official Documentation: https://keras.io/examples/nlp/masked_language_modeling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7p3mALxDVogZ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the function which creates automatic labels for our dataset\n",
    "by using the vectorization and vocab layer we created previously\n",
    "'''\n",
    "def get_masked_input_and_labels(encoded_texts, mask_token_id):\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAcU43MJ6acA"
   },
   "source": [
    "### CREATE AUTOMATIC LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUKFIf-z6fZE"
   },
   "source": [
    "ADHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtyTr9wa6cg5"
   },
   "outputs": [],
   "source": [
    "# Prepare data for masked language model for the unlabeled ADHD dataset\n",
    "x_all_adhd = adhd_vectorize_layer(cleaned_adhd_docs).numpy()\n",
    "x_masked_adhd_train, y_masked_adhd_labels, adhd_sample_weights = get_masked_input_and_labels(\n",
    "    x_all_adhd, adhd_mask_token_id)\n",
    "\n",
    "# formulate our new self-labeled dataset\n",
    "mlm_adhd_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_adhd_train, y_masked_adhd_labels, adhd_sample_weights))\n",
    "mlm_adhd_ds = mlm_adhd_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHkFy4QY6hr5"
   },
   "source": [
    "ANXIETY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtWboVAr6h3p"
   },
   "outputs": [],
   "source": [
    "# Prepare data for masked language model for the unlabeled anxiety dataset\n",
    "x_all_anxiety = anxiety_vectorize_layer(cleaned_anxiety_docs).numpy()\n",
    "x_masked_anxiety_train, y_masked_anxiety_labels, anxiety_sample_weights = get_masked_input_and_labels(\n",
    "    x_all_anxiety, anxiety_mask_token_id)\n",
    "\n",
    "# formulate our new self-labeled dataset\n",
    "mlm_anxiety_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_anxiety_train, y_masked_anxiety_labels, anxiety_sample_weights))\n",
    "mlm_anxiety_ds = mlm_anxiety_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs1SsLDy6iF1"
   },
   "source": [
    "ALCOHOLISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHbOnVn66iVT"
   },
   "outputs": [],
   "source": [
    "# Prepare data for masked language model for the unlabeled alcoholism dataset\n",
    "x_all_alcoholism = alcoholism_vectorize_layer(cleaned_alcoholism_docs).numpy()\n",
    "x_masked_alcoholism_train, y_masked_alcoholism_labels, alcoholism_sample_weights = get_masked_input_and_labels(\n",
    "    x_all_alcoholism, alcoholism_mask_token_id)\n",
    "\n",
    "# formulate our new self-labeled dataset\n",
    "mlm_alcoholism_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_alcoholism_train, y_masked_alcoholism_labels, alcoholism_sample_weights))\n",
    "mlm_alcoholism_ds = mlm_alcoholism_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EJTujks6mli"
   },
   "source": [
    "DEPRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAmfoiZ3u_f-"
   },
   "source": [
    "NOTE: colab crashes for this big dataset due to lack of ram\n",
    "\n",
    "TODO: try to run this notebook on high ram datahub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALI6qgkQ6muU"
   },
   "outputs": [],
   "source": [
    "# # Prepare data for masked language model for the unlabeled depression dataset\n",
    "# x_all_depression = depression_vectorize_layer(cleaned_depression_docs).numpy()\n",
    "# x_masked_depression_train, y_masked_depression_labels, depression_sample_weights = get_masked_input_and_labels(\n",
    "#     x_all_depression, depression_mask_token_id)\n",
    "\n",
    "# # formulate our new self-labeled dataset\n",
    "# mlm_depression_ds = tf.data.Dataset.from_tensor_slices(\n",
    "#     (x_masked_depression_train, y_masked_depression_labels, depression_sample_weights))\n",
    "# mlm_depression_ds = mlm_depression_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spOXyngEY2Dg"
   },
   "source": [
    "We now have a labeled ADHD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvUtuh1o14y6",
    "outputId": "01208a5f-99ca-473e-ee3e-85e15fcd788d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adhd 1426\n",
      "anxiety 1803\n",
      "alcoholism 185\n"
     ]
    }
   ],
   "source": [
    "# length of our data\n",
    "print('adhd', len(mlm_adhd_ds))\n",
    "print('anxiety', len(mlm_anxiety_ds))\n",
    "print('alcoholism', len(mlm_alcoholism_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lECLUOm0ZOF3"
   },
   "source": [
    "## Create BERT model (Pretraining Model) for masked language modeling\n",
    "\n",
    "It will take token ids as inputs (including masked tokens) and it will predict the correct ids for the masked input tokens.\n",
    "\n",
    "Code sample from Keras Official Documentation: https://keras.io/examples/nlp/masked_language_modeling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBr9cKRhY8z0"
   },
   "outputs": [],
   "source": [
    "## please note this bert module is from Keras Documentation\n",
    "## it is included here because tensorflow or keras do not have a set of\n",
    "## functions we can just use for this, we have to include them here\n",
    "\n",
    "def bert_module(query, key, value, i):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}/multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_{}/ffn\".format(i),\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",\n",
    "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "id2token = dict(enumerate(alcoholism_vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}\n",
    "mask_token_id = alcoholism_mask_token_id\n",
    "\n",
    "# anxiety_id2token = dict(enumerate(anxiety_vectorize_layer.get_vocabulary()))\n",
    "# anxiety_token2id = {y: x for x, y in anxiety_id2token.items()}\n",
    "\n",
    "# alcoholism_id2token = dict(enumerate(alcoholism_vectorize_layer.get_vocabulary()))\n",
    "# alcoholism_token2id = {y: x for x, y in alcoholism_id2token.items()}\n",
    "\n",
    "# depression_id2token = dict(enumerate(depression_vectorize_layer.get_vocabulary()))\n",
    "# depression_token2id = {y: x for x, y in depression_id2token.items()}\n",
    "\n",
    "\n",
    "\n",
    "## optional text generator\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vgvg8Q7TaEo",
    "outputId": "6d559948-43e3-47c5-e359-db8fe26f556c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)     (None, 256, 128)     3840000     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 256, 128)    0           ['word_embedding[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattention (  (None, 256, 128)    66048       ['tf.__operators__.add_6[0][0]', \n",
      " MultiHeadAttention)                                              'tf.__operators__.add_6[0][0]', \n",
      "                                                                  'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 256, 128)    0           ['tf.__operators__.add_6[0][0]', \n",
      " mbda)                                                            'encoder_0/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_7[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)     (None, 256, 128)     33024       ['encoder_0/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 256, 128)    0           ['encoder_0/att_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_0/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_8[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)                (None, 256, 30000)   3870000     ['encoder_0/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,809,584\n",
      "Trainable params: 7,809,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## this callback can show us the evolution of our training\n",
    "adhd_sample_tokens = adhd_vectorize_layer([\"Lately I have been feeling [mask] and I do not know what to do\"])\n",
    "generator_callback = MaskedTextGenerator(adhd_sample_tokens.numpy())\n",
    "\n",
    "bert_masked_adhd_model = create_masked_language_bert_model()\n",
    "bert_masked_adhd_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbbbceS32qQA",
    "outputId": "50e9755a-c719-4268-e361-de92bae67a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)     (None, 256, 128)     3840000     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 256, 128)    0           ['word_embedding[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattention (  (None, 256, 128)    66048       ['tf.__operators__.add_9[0][0]', \n",
      " MultiHeadAttention)                                              'tf.__operators__.add_9[0][0]', \n",
      "                                                                  'tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 256, 128)    0           ['tf.__operators__.add_9[0][0]', \n",
      " ambda)                                                           'encoder_0/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_10[0][0]']\n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)     (None, 256, 128)     33024       ['encoder_0/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 256, 128)    0           ['encoder_0/att_layernormalizatio\n",
      " ambda)                                                          n[0][0]',                        \n",
      "                                                                  'encoder_0/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_11[0][0]']\n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)                (None, 256, 30000)   3870000     ['encoder_0/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,809,584\n",
      "Trainable params: 7,809,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#bert model on a much smaller dataset\n",
    "\n",
    "## this callback can show us the evolution of our training\n",
    "alcoholism_sample_tokens = alcoholism_vectorize_layer([\"I am so happy to be [mask] now. Daily drinking was ruining my life.\"])\n",
    "generator_callback = MaskedTextGenerator(alcoholism_sample_tokens.numpy())\n",
    "\n",
    "bert_masked_alcoholism_model = create_masked_language_bert_model()\n",
    "bert_masked_alcoholism_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQnftL2BbB05"
   },
   "source": [
    "## Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_scy1WQ9bBVV"
   },
   "outputs": [],
   "source": [
    "# unfortunately takes like 15 hours\n",
    "\n",
    "# bert_masked_adhd_model.fit(mlm_adhd_ds, epochs=2, callbacks=[generator_callback])\n",
    "# bert_masked_adhd_model.save(data_path + \"/adhd/bert_mlm_adhd.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZLb-ALl3QLv"
   },
   "source": [
    "train on the alcoholism dataset, much smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNTzdkRP3PLJ"
   },
   "outputs": [],
   "source": [
    "# unfortunately takes like 15 hours\n",
    "\n",
    "# bert_masked_alcoholism_model.fit(mlm_alcoholism_ds, epochs=2, callbacks=[generator_callback])\n",
    "# bert_masked_alcoholism_model.save(data_path + \"/alcoholism/bert_mlm_alcoholism.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e0P2kmYHlM2",
    "outputId": "8905961a-9b42-409f-ea32-149526258662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text vector\n",
      " [ 4472    29  9761   176    84   528  4267  7077 13939     1     1  4267\n",
      "  5831  4472   456  4267   197    33   806   306   345   505  1018 18690\n",
      "  5300  4472   720  1860  7740    16  1319  2176    74     8     2   800\n",
      "     6  5300   305   290    42   110   200   206     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "label\n",
      " [ 4472    29  9761   176    84   528    19  7077 13939     1     1   176\n",
      "  5831  4472   456     5   197    33   806   306   345   505  1018 18690\n",
      "  5300  4472   720   482  7740    16  1319  2176    74     8     2   800\n",
      "     6  5300   305   290    42   110   200   206     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch, weights in mlm_adhd_ds.take(1):\n",
    "    for i in range(1):\n",
    "        print('text vector\\n', text_batch.numpy()[i])\n",
    "        print('label\\n', label_batch.numpy()[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPTVtARiygFR"
   },
   "source": [
    "### Split Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f57M8CNtymtX",
    "outputId": "8d23a594-9cab-47cf-aef6-ffd945886a67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1426"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our model previously split 32% of the data for this auto-classification task\n",
    "\n",
    "DATASET_SIZE = len(list(mlm_adhd_ds))\n",
    "DATASET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fXZLT4uUwTI",
    "outputId": "29d83d38-e47d-442a-ebb3-9dbe62116477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 998\n",
      "test 428\n"
     ]
    }
   ],
   "source": [
    "# this piece of code lets us control the train/test split\n",
    "# it splits the adhd tensorflow dataset and splits it\n",
    "\n",
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "test_size = int(0.3 * DATASET_SIZE)\n",
    "\n",
    "adhd_train_dataset = mlm_adhd_ds.take(train_size)\n",
    "adhd_test_dataset = mlm_adhd_ds.skip(train_size)\n",
    "\n",
    "print(\"train\", len(list(adhd_train_dataset)))\n",
    "print(\"test\", len(list(adhd_test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjG3mWreYSpD",
    "outputId": "fee4c54d-7b79-41a5-9861-c2d273d55984"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5279,  8827,  4267, ...,     0,     0,     0],\n",
       "        [  275, 27435,  4267, ...,     0,     0,     0],\n",
       "        [    2,  3000,    19, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 1766,   436,  4267, ...,  3258,   184,   206],\n",
       "        [    3,   107,     3, ...,     0,     0,     0],\n",
       "        [  213,    40,    49, ...,     0,     0,     0]]),\n",
       " array([[ 1503,    46,  1367, ...,     0,     0,     0],\n",
       "        [    4,   444,    72, ...,     0,     0,     0],\n",
       "        [   10,   361,  2219, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [   29,   186,   813, ...,     0,     0,     0],\n",
       "        [  882,   190,   379, ...,     0,     0,     0],\n",
       "        [  184,  2710, 10017, ...,     0,     0,     0]]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split tensorflow datasets into x and y lists to use for sklearn\n",
    "\n",
    "adhd_train_y = []\n",
    "adhd_train_x = []\n",
    "\n",
    "# training data split into text vectorizations and vectorized labels\n",
    "adhd_train_x = np.array([list(x[0].numpy()) for x in list(adhd_train_dataset)])\n",
    "adhd_train_y = np.array([x[1].numpy() for x in list(adhd_train_dataset)])\n",
    "\n",
    "adhd_train_x[0], adhd_train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HljJO7DActMm",
    "outputId": "8f647381-8989-4ad2-b75a-d457709297aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(998, 998)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just view the length of our dataset, make sure its the right number of training samples\n",
    "\n",
    "len(adhd_train_x), len(adhd_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKcbDHo_qHjL",
    "outputId": "7d713c90-554b-4a37-e682-29afb3e2666f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((998, 32, 256), (998, 32, 256))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM\n",
    "# we have a dataset with a very weird shape, making it difficult to put into\n",
    "# any sklearn fit() function. We need to reduce the dimensionality of the data\n",
    "# somehow to be able to train with it\n",
    "\n",
    "np.array(adhd_train_x).shape, np.array(adhd_train_y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX_wPP7ousbT"
   },
   "source": [
    "TODO: next step would be to just throw the training set into a sklearn algo and then\n",
    "predict on the test set. The problem here is the shape of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSrbRGT5sQPN"
   },
   "source": [
    "kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "5AOEqCFnsPdW",
    "outputId": "7778f337-af03-4f13-9b1f-53443834efc5"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-08fecb6a8e8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madhd_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madhd_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m             raise ValueError(\n\u001b[1;32m    795\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m             )\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(adhd_train_x, adhd_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAFzYWFrWkgr"
   },
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "QbQGgIE0Wje7",
    "outputId": "79572488-a9db-4a11-8589-5b992487db04"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-f971f9936f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madhd_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madhd_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             )\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m             raise ValueError(\n\u001b[1;32m    795\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m             )\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "\n",
    "clf.fit(adhd_train_x, adhd_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-0AxYBO1th_B",
    "outputId": "e3c5f29a-efb9-495d-bf9b-1c4caa0d281b"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-2ce27fa05a82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlin_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madhd_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madhd_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# overall.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mof\u001b[0m \u001b[0mCSR\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (array([[[ 1503,    46,  1367, ...,     0,     0,     0],\n        [    4,   444,    72, ...,     0,     0,     0],\n        [   10,   361,  2219, ...,     0,     0,     0],\n        ...,\n        [   29,   186,   813, ...,     0,     0,     0],\n        [  882,   190,   379, ...,     0,     0,     0],\n        [  184,  2710, 10017, ...,     0,     0,     0]],\n\n       [[ 1633,  1191,     9, ...,     0,     0,     0],\n        [  413,  1447,   197, ...,     0,     0,     0],\n        [  620,   346,    29, ...,     0,     0,     0],\n        ...,\n        [ 1032,  9601,   871, ...,     0,     0,     0],\n        [   26,    21,   189, ...,     0,     0,     0],\n        [   26,   210,    77, ...,     0,     0,     0]],\n\n       [[    2,   372,  6857, ...,     0,     0,     0],\n        [   89,   509,  1069, ...,     0,     0,     0],\n        [   10,   236,     4, ...,     0,     0,     0],\n        ...,\n        [  966,  1659,     2, ...,     0,     0,     0],\n        [  196,  4255,  1188, ...,     0,     0,     0],\n        [  192,     6,   677, ...,     0,     0,     0]],\n\n       ...,\n\n       [[  604,  3724,  1124, ...,     0,     0,     0],\n        [  116,     4,    31, ...,     0,     0,     0],\n        [  252,   732,    49, ...,     0,     0,     0],\n        ...,\n        [ 1423,   827,    55, ...,     0,     0,     0],\n        [    2,  1214,   431, ...,     0,     0,     0],\n        [   88,  2045,   893, ...,     0,     0,     0]],\n\n       [[  122,   164,     2, ...,     0,     0,     0],\n        [ 2882,   393,  4048, ...,     0,     0,     0],\n        [   10,    41,  2565, ...,    49,   280,     2],\n        ...,\n        [10961,   135,  1671, ...,     0,     0,     0],\n        [23597,    35,   263, ...,     0,     0,     0],\n        [   54,    39,     4, ...,     0,     0,     0]],\n\n       [[   27,   300,  6346, ...,     0,     0,     0],\n        [   53,   275,    43, ...,     0,     0,     0],\n        [  213,   899,     2, ...,     0,     0,     0],\n        ...,\n        [    2,    63,  2426, ...,     0,     0,     0],\n        [  188,    37,   205, ...,     0,     0,     0],\n        [ 1093,  4315,  2100, ...,     0,     0,     0]]]),)"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lin_svm = OneVsRestClassifier(LinearSVC(random_state=0)).fit(adhd_train_x, adhd_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aybma-HDKXPu"
   },
   "source": [
    "# Create a small hand-labelled dataset to test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW4g9rphKqq7"
   },
   "outputs": [],
   "source": [
    "''' \n",
    "  this step requires repeating the tensorflow dataset creation from earlier\n",
    "  this time the labels are correct, and we are selecting a much smaller\n",
    "  random subset of the data for training, which is hand-labeled\n",
    "'''\n",
    "\n",
    "\n",
    "random.sample(cleaned_adhd_docs, n)\n",
    "\n",
    "# make the adhd features into a tensor\n",
    "adhd_features = tf.constant(cleaned_adhd_docs)\n",
    "adhd_true_labels = tf.constant([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lesErrPdREVX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "COGS 118A Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
