{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning on Social Media Posts: Mental Health Disorder Classification\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Gilberto Robles\n",
    "- Soyon Kim\n",
    "- Allan Tan\n",
    "- Jason Sheu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "Mental health patients struggle with financial, psychological, and logistical burden when seeking professional help. As such, social media, particularly Reddit, has become a popular outlet for people to ask questions anonymously for help. In order to aid mental healthcare become accessible and affordable online, we aim to use supervised machine learning to detect self-harming and destructive behavior as well as classify potential mental disorders using the DSM-5 (Diagnostic and Statistical Manual of Mental Disorders) Diagnostic Criteria. Our model could also be utilized in a professional environment by assisting care providers with diagnostic information. Additionally, the practicality of this experiment would extend into the implementation of notification alerts to reddit users privately, if their posts are detected to have a potential for self-harm or a diagnosis of a mental disorder. \n",
    "\n",
    "We use the Reddit Mental Health Dataset, which consists of posts from 28 different subreddits (15 mental health support groups) from 2018-2020. In our methods, we deploy a Self-Supervised Text-Classification model with TensorFlow to allow us to create a labeled dataset. Further, we implement two Multi-Class Text Classification models in a cross-examination study to evaluate the performance difference between K-Nearest Neighbors and Support Vector Machines to find patterns in people seeking support in mental health related subreddits. The posts will thus be related to mental disorders and harmful behavior in order to potentially diagnose (or at least warn) users about the content of their posts, and then direct them to helpful resources in an accessible, private, and preventative manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "### The Mental Health Epidemic\n",
    "The current mental healthcare system places various financial, psychological, and logistical burdens on those seeking professional help for their mental disorders.\n",
    "To list a few:\n",
    "- There is a huge shortage of therapists/psychiatrists, leading to months and even year-long waitlists for an incoming patient’s first appointment. Especially since many do not accept insurance, this causes a huge difficulty in patients finding connection to clinicians in the first place.\n",
    "- There is a strong social stigma against seeking professional help for mental disorders. That is, the fear of admitting to one’s issues and becoming labeled as “disabled” leads to anosognosia, or the denial or “lack of insight” in acknowledging one’s mental health issues.\n",
    "- There is also the burden on incoming patients to find the best-fitting therapist in terms of location, specialization, cost, gender, age, and culture. When switching clinicians, the psychological burden of repeated intake sessions where one must elaborate on their mental health history can also be extremely cumbersome.\n",
    "\n",
    "### Previous Research: Machine Learning and Mental Health\n",
    "As a result of this lack of outlet for mental health struggles, many turn to the internet to anonymously confess their difficulties and build communities for support. Low et al. for example, the developer of our dataset for this project, performed an an observational study with natural language processing to reveal vulnerable mental health support groups and heightened health anxiety on Reddit during COVID-19. Meanwhile, Human-Machine Interaction (HMI) is the field that explores computer and robot technology that focuses on the relationship between people and machines. As closely related to humans, HMI has focused on enhancing human health, particularly mental health. \n",
    "\n",
    "For example, \n",
    "- Chikersal et al. developed automatic Depression detection through machine learning of biosensor feedback [2]. There are also various products in the market that use machine learning and other intelligent methods for user personalization in preference and treatment. For example, there are mobile apps including mindfulness apps like Headspace, Calm, and UCLA Mindful and therapeutic robots like Paro, Hugvie, Pepper, Carebot, and QTRobot [4, 5].\n",
    "- Cheng et al. created \"Psychologist in a Pocket\", a Lexicon Development and Content Validation of a Mobile-Based App for Depression Screening\n",
    "\n",
    "For the development of our project and our own lexicon, we also consulted Li et al. Automatic Construction of a Depression-Domain Lexicon Based on Microblogs: Text Mining Study, as well as \"Lexicon-based method to detect signs of depression in text\" on GitHub by Pablo Gamallo.\n",
    "\n",
    "Additionally, during the development of our Self-Supervised Model for text classification, we consulted with Keras documentation, \"End-to-end Masked Language Modeling with BERT\" to train a language model in a self-supervised setting (without human-annotated labels).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "In this project, we want to tackle the problem of lack of accessible, affordable, and preventative mental health support resources through the use of popular social media sites. Specifically, we want to assist Internet users struggling with mental disorders who have exhibited a range of qualifying behavior traits, as per DSM-5, through the use of an automated system that classifies them based on their Reddit posts. The most challenging aspect of this, is that our dataset did not come pre-labeled for ease of access, and every resource we consulted had private lexicon datasets, which were not accessible to us. For this reason we decided to develop a Masked Language Model to self-supervise our data, and create automatic labels, which we would later divide into training and testing to compare different model performances.\n",
    "\n",
    "The system would learn words or phrases commonly used by people with the qualifying criteria versus people who do not exhibit any concern within the same Reddit thread, with the use of KNN and SVM text classification models. That is, the system would learn words and phrases used by people who clearly exhibit behavior and meet the criteria for mental disorders. Then, given a reddit post, the system will try to detect whether the post meets concerning criteria, at which point the user will be notified and directed to relevant resouces. For this project, we aim to primarily focus on a single model that can differentiate between people that display signs of mental illness versus those that do not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Link to dataset: https://zenodo.org/record/3941387/files/depression_2018_features_tfidf_256.csv?download=1\n",
    "\n",
    "Dataset size: 24535 rows, 350 columns. Many of these columns are tf-idf statistics that we will not be using. The column that we are interested in is primarily the 'post' column, which is the column containing the text of the post made by a user.\n",
    "\n",
    "The dataset we plan to use is text that was scraped from Reddit’s mental health support subreddits. The link to the dataset can be found as item 3 in the footnotes [3]. The data was originally created to examine the effects of COVID-19 on mental health. It contains posts from 28 different subreddits, or 15 mental health support groups, and dates range from 2018-2020. The link provided includes a variety of mental health disorders.\n",
    "\n",
    "For our project in particular, we will be analyzing the posts associated with self-harming behavior as per DSM-5 criteria. As a result, the data we will be examining are for example, in the case of \"Major depressive disorder\", would be\n",
    "- depression_2018_features_tfidf_256.csv\n",
    "- depression_2019_features_tfidf_256.csv\n",
    "- etc.\n",
    "\n",
    "Regarding these two datasets in particular, the each have about 24500 and 33500 observations, respectively. They also have a wide array of variables concerning the text and the text’s metadata. Examples include:\n",
    "- the subreddit the text was scraped from\n",
    "- the username of the poster\n",
    "- actual text itself\n",
    "- date posted\n",
    "- unique words\n",
    "- syllables\n",
    "\n",
    "In total there are 350 variables for one observation. Similarly, a single observation basically constitutes a post on the subreddit would have all of the aforementioned variables. Because our project revolves around textual analysis and classification, we will predominantly focus on the reddit post's text portion of each observation. As a result, some critical variables are:\n",
    "- the text of the post\n",
    "- the date it was posted\n",
    "- number of words\n",
    "- number of times a “trigger” word such as gun or suicide is used, etc.\n",
    "\n",
    "The text of the post will be in string format and the date will also be a string in MM/DD/YYYY format. Other variables will be in integer or float format as they are primarily responsible for keeping track of word frequencies. Because we are only using a few of the variables out of the 350 available, we will need to remove the unnecessary ones. Additionally, to preserve anonymity, we will be removing the author variable from each observation. Some additional cleaning could also take the form of removing special characters from the text of each observation or making all characters lowercase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Internet users have been posting volumes of texts discussing their mental illness or self harm urges. In this project, in order to solve the problem of lack of accessible, affordable and preventative mental health support resources, we aim to develop a mental disorder detection system and provide help to users by providing automatic warnings about the onset of hamrful behavior described explicitly by the user.Specifically, we will build a supervised machine learning (ML)-based system that detects depressive text and potentially the onset of depression. \n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "##### Data Splitting and Cleaning\n",
    "The first thing to tackle this difficult dataset is to do preprocessing on the data. We will take 4 sub-datasets with tens of thousands of samples each, namely: ADHD, Anxiety, Alcoholism, and Depression. \n",
    "\n",
    "Since each data point comes with extra information we will not be using, we extract only the reddit post text that we will be using in our analysis. \n",
    "\n",
    "Since this is a text NLP task, we need to clean the data of unwanted stopwords which are irrelevant in understanding the sentiment of text. We also remove the capitalizaion and symbols.\n",
    "\n",
    "\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "The original plan for our project was to use TF-IDF as an information retrieval (IR) tool to quantify the importance or relevance of words or phrases, in comparison to their relevance scores in pre-labeled mental disorder lexicons. Unfortunately the lexicons that we found from researchers in the same field, had their results restricted for public use and we were not able to move on with this stage alone as a pre-classification task. While the text vectorization remains relevant for establishing a representative vocabulary from the training data, we decided to take a step further.\n",
    "\n",
    "\n",
    "### BERT-Like Masked Language Model for Self-Supervised Text Classification\n",
    "\n",
    "Since our only options from this point was to either give up on the project up to the previous step, manually hand-label thousands of samples from our data, or to escalate into a Neural Network layer, we decided to do the latter.\n",
    "\n",
    "In this stage we create a pretraining model with the Keras TextVectorization and MultiHeadAttention layers to create a BERT Transformer-Encoder network architecture which automatically labels the datasets. By extrating the token IDs from our vectorization as inputs (including masked tokens) the model predicts the correct IDs for the masked input tokens.\n",
    "\n",
    "### Training and Testing with KNN and SVM\n",
    "\n",
    "In this section our goal is to create a cross-examination study to compare the classification accuracy of K Nearest Neighbors vs a non-linear Support Vector Machine. Since Neural Networks take hours and even days to train, we only used a simple vocabulary layer to pretrain or model with, but to generalize these results to the entire Reddit Mental Health Disorder dataset, we opt for faster and more efficient classification models. In the model selection process we compare KNN and SVM.\n",
    "\n",
    "### Machine Learning and Online Mental Health Support\n",
    "The end goal of this project essentially is to make use of such a classification model to create a private and optional alert and support system for people who might be looking online for mental health support. By being able to correctly classify who could use this help, mental health can become more widely accessible for those who are already searching. And we can help direct them in the right direction with disorder-specific information. Our proposed solution would be an optional and private alert through Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Data Splitting and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plz add our preprocessing code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "We struggled through a lot of difficulties during the process of this project. Although we did not end up using our TF-IDF direct results, we are including them here because they are the basis on which we later built our other very similar text vectorization layer in the next section. Although it was not a direct predecessor to the following steps in the project, it established a useful foundation for the structure of the next vectorization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plz add our tf idf code here WITH VISUALIZATIONS!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-Like Masked Language Model for Self-Supervised Text Classification\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plz add our self supervised code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
