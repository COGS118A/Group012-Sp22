{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning on Social Media Posts: Mental Health Disorder Classification\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Gilberto Robles\n",
    "- Soyon Kim\n",
    "- Allan Tan\n",
    "- Jason Sheu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mental health patients struggle with financial, psychological, and logistical burden when seeking professional help. As such, social media, particularly Reddit, has become a popular outlet for people to anonymously seek help. In order to help make mental healthcare accessible and affordable online, we aim to use supervised machine learning to detect self-harming and destructive behavior as well as classify potential mental disorders using the DSM-5 (Diagnostic and Statistical Manual of Mental Disorders) Diagnostic Criteria. Our model could also be utilized in a professional environment by assisting care providers with diagnostic information. We use the Reddit Mental Health Dataset, which consists of posts from 28 different subreddits (15 mental health support groups) from 2018-2020. In our methods, we deploy a Multi-Class Text Classification model in a cross-examination study to evaluate the performance difference with K-Nearest Neighbors and Support Vector Machines to find patterns in people seeking support in mental health related subreddits. The posts will thus be related to mental disorders and harmful behavior in order to potentially diagnose (or at least warn) users about the content of their posts, and then direct them to helpful resources in an accessible, private, and preventative manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "### The Mental Health Epidemic\n",
    "The current mental healthcare system places various financial, psychological, and logistical burdens on those seeking professional help for their mental disorders.  \n",
    "\n",
    "To list a few:\n",
    "- There is a huge shortage of therapists/psychiatrists, leading to months and even year-long waitlists for an incoming patient’s first appointment. Especially since many do not accept insurance, this causes a huge difficulty in patients finding connection to clinicians in the first place.\n",
    "- There is a strong social stigma against seeking professional help for mental disorders. That is, the fear of admitting to one’s issues and becoming labeled as “disabled” leads to anosognosia, or the denial or “lack of insight” in acknowledging one’s mental health issues.\n",
    "- There is also the burden on incoming patients to find the best-fitting therapist in terms of location, specialization, cost, gender, age, and culture. When switching clinicians, the psychological burden of repeated intake sessions where one must elaborate on their mental health history can also be extremely cumbersome.\n",
    "\n",
    "### Previous Research: Machine Learning and Mental Health\n",
    "As a result of this lack of outlet for mental health struggles, many turn to the internet to anonymously confess their difficulties and build communities for support. For example, Reddit has user-established mental health support groups for various mental health disorders such as addiction, alcoholism, Bipolar disorder, anxiety, depression, eating disorders, and post-traumatic stress disorder. Meanwhile, Human-Machine Interaction (HMI) is the field that explores computer and robot technology that focuses on the relationship between people and machines. As closely related to humans, HMI has focused on enhancing human health, particularly mental health. \n",
    "\n",
    "For example, \n",
    "- Chikersal et al. developed automatic Depression detection through machine learning of biosensor feedback [2]. There are also various products in the market that use machine learning and other intelligent methods for user personalization in preference and treatment. For example, there are mobile apps including mindfulness apps like Headspace, Calm, and UCLA Mindful and therapeutic robots like Paro, Hugvie, Pepper, Carebot, and QTRobot [4, 5].\n",
    "- Cheng et al. created \"Psychologist in a Pocket\", a Lexicon Development and Content Validation of a Mobile-Based App for Depression Screening\n",
    "\n",
    "For the development of our project and our own lexicon, we also consulted Li et al. Automatic Construction of a Depression-Domain Lexicon Based on Microblogs: Text Mining Study, as well as \"Lexicon-based method to detect signs of depression in text\" on GitHub by Pablo Gamallo.\n",
    "\n",
    "Additionally, during the development of our Self-Supervised Model for text classification, we consulted with Keras documentation, \"End-to-end Masked Language Modeling with BERT\" to train a language model in a self-supervised setting (without human-annotated labels).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "In this project, we want to tackle the problem of lack of accessible, affordable, and preventative mental health support resources through the use of popular social media sites. Specifically, we want to assist Internet users struggling with mental disorders who have exhibited a range of qualifying behavior traits, as per DSM-5, through the use of an automated system that classifies them based on their Reddit posts. The system would learn words or phrases commonly used by people with the qualifying criteria versus people who do not exhibit any concern within the same Reddit thread, with the use of KNN and SVM text classification models. That is, the system would learn words and phrases used by people who clearly exhibit behavior and meet the criteria for mental disorders. Then, given a reddit post, the system will try to detect whether the post meets concerning criteria, at which point the user will be notified and directed to relevant resouces. For this project, we aim to primarily focus on a single model that can differentiate between people that display signs of mental illness versus those that do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Link to dataset: https://zenodo.org/record/3941387/files/depression_2018_features_tfidf_256.csv?download=1\n",
    "\n",
    "Dataset size: 24535 rows, 350 columns. Many of these columns are tf-idf statistics that we will not be using. The column that we are interested in is primarily the 'post' column, which is the column containing the text of the post made by a user.\n",
    "\n",
    "The dataset we plan to use is text that was scraped from Reddit’s mental health support subreddits. The link to the dataset can be found as item 3 in the footnotes [3]. The data was originally created to examine the effects of COVID-19 on mental health. It contains posts from 28 different subreddits, or 15 mental health support groups, and dates range from 2018-2020. The link provided includes a variety of mental health disorders.\n",
    "\n",
    "For our project in particular, we will be analyzing the posts associated with self-harming behavior as per DSM-5 criteria. As a result, the data we will be examining are for example, in the case of \"Major depressive disorder\", would be\n",
    "- depression_2018_features_tfidf_256.csv\n",
    "- depression_2019_features_tfidf_256.csv\n",
    "- etc.\n",
    "\n",
    "Regarding these two datasets in particular, the each have about 24500 and 33500 observations, respectively. They also have a wide array of variables concerning the text and the text’s metadata. Examples include:\n",
    "- the subreddit the text was scraped from\n",
    "- the username of the poster\n",
    "- actual text itself\n",
    "- date posted\n",
    "- unique words\n",
    "- syllables\n",
    "\n",
    "In total there are 350 variables for one observation. Similarly, a single observation basically constitutes a post on the subreddit would have all of the aforementioned variables. Because our project revolves around textual analysis and classification, we will predominantly focus on the reddit post's text portion of each observation. As a result, some critical variables are:\n",
    "- the text of the post\n",
    "- the date it was posted\n",
    "- number of words\n",
    "- number of times a “trigger” word such as gun or suicide is used, etc.\n",
    "\n",
    "The text of the post will be in string format and the date will also be a string in MM/DD/YYYY format. Other variables will be in integer or float format as they are primarily responsible for keeping track of word frequencies. Because we are only using a few of the variables out of the 350 available, we will need to remove the unnecessary ones. Additionally, to preserve anonymity, we will be removing the author variable from each observation. Some additional cleaning could also take the form of removing special characters from the text of each observation or making all characters lowercase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Internet users have been posting volumes of texts discussing their mental illness or self harm urges. In this project, in order to solve the problem of lack of accessible, affordable and preventative mental health support resources, we aim to develop a mental disorder detection system and provide help to users by providing automatic warnings about the onset of hamrful behavior described explicitly by the user. \n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "##### Data Splitting and Cleaning\n",
    "The first thing to tackle this difficult dataset is to do preprocessing on the data. We will take 4 sub-datasets with tens of thousands of samples each, namely: ADHD, Anxiety, Alcoholism, and Depression. \n",
    "\n",
    "Since each data point comes with extra information we will not be using, we extract only the reddit post text that we will be using in our analysis. \n",
    "\n",
    "Since this is a text NLP task, we need to clean the data of unwanted stopwords which are irrelevant in understanding the sentiment of text. We also remove the capitalizaion and symbols.\n",
    "\n",
    "\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "The original plan for our project was to use TF-IDF as an information retrieval (IR) tool to quantify the importance or relevance of words or phrases, in comparison to their relevance scores in pre-labeled mental disorder lexicons. Unfortunately the lexicons that we found from researchers in the same field, had their results restricted for public use and we were not able to move on with this stage alone as a pre-classification task. While the text vectorization remains relevant for establishing a representative vocabulary from the training data, we decided to take a step further.\n",
    "\n",
    "\n",
    "### BERT-Like Masked Language Model for Self-Supervised Text Classification\n",
    "\n",
    "Since our only options from this point was to either give up on the project up to the previous step, manually hand-label thousands of samples from our data, or to escalate into a Neural Network layer, we decided to do the latter.\n",
    "\n",
    "In this stage we create a pretraining model with the Keras TextVectorization and MultiHeadAttention layers to create a BERT Transformer-Encoder network architecture which automatically labels the datasets. By extrating the token IDs from our vectorization as inputs (including masked tokens) the model predicts the correct IDs for the masked input tokens.\n",
    "\n",
    "### Training and Testing with KNN and SVM\n",
    "\n",
    "In this section our goal is to create a cross-examination study to compare the classification accuracy of K Nearest Neighbors vs a non-linear Support Vector Machine. Since Neural Networks take hours and even days to train, we only used a simple vocabulary layer to pretrain or model with, but to generalize these results to the entire Reddit Mental Health Disorder dataset, we opt for faster and more efficient classification models. In the model selection process we compare KNN and SVM.\n",
    "\n",
    "### Machine Learning and Online Mental Health Support\n",
    "The end goal of this project essentially is to make use of such a classification model to create a private and optional alert and support system for people who might be looking online for mental health support. By being able to correctly classify who could use this help, mental health can become more widely accessible for those who are already searching. And we can help direct them in the right direction with disorder-specific information. Our proposed solution would be an optional and private alert through Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "At least one evaluation metric we will be using to quantify the performance of our model is precsion, recall and f1 score. Because we plan on leveraging sentiment analysis on the textual content of each observation, we will likely begin with logistic regression as a baseline.  \n",
    "Precision, recall, and f1 scores matter to us as we are performing classification, and want to minimize the amount of false positives and negatives, using precision and recall to check the performance of our model. We will use f1 score as an overall indicator of model accuracy, as well as regular accuracy between testing and training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADHD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from drive\n",
    "data_path = '.'\n",
    "adhd_2018_df = pd.read_csv(data_path + '/adhd/adhd_2018.csv')\n",
    "adhd_2019_df = pd.read_csv(data_path + '/adhd/adhd_2019.csv')\n",
    "adhd_post_df = pd.read_csv(data_path + '/adhd/adhd_post.csv')\n",
    "adhd_pre_df  = pd.read_csv(data_path + '/adhd/adhd_pre.csv')\n",
    "\n",
    "# join all data into one DataFrame\n",
    "adhd_dataset = pd.concat([adhd_2018_df, adhd_2019_df, adhd_post_df, adhd_pre_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anxiety Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from drive\n",
    "anxiety_2018_df = pd.read_csv(data_path + '/anxiety/anxiety_2018.csv')\n",
    "anxiety_2019_df = pd.read_csv(data_path + '/anxiety/anxiety_2019.csv')\n",
    "anxiety_post_df = pd.read_csv(data_path + '/anxiety/anxiety_post.csv')\n",
    "anxiety_pre_df  = pd.read_csv(data_path + '/anxiety/anxiety_pre.csv')\n",
    "\n",
    "# join all data into one DataFrame\n",
    "anxiety_dataset = pd.concat([anxiety_2018_df, anxiety_2019_df, anxiety_post_df, anxiety_pre_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alcoholism Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from drive\n",
    "alcoholism_2018_df = pd.read_csv(data_path + '/alcoholism/alcoholism_2018.csv')\n",
    "alcoholism_2019_df = pd.read_csv(data_path + '/alcoholism/alcoholism_2019.csv')\n",
    "alcoholism_post_df = pd.read_csv(data_path + '/alcoholism/alcoholism_post.csv')\n",
    "alcoholism_pre_df  = pd.read_csv(data_path + '/alcoholism/alcoholism_pre.csv')\n",
    "\n",
    "# join all data into one DataFrame\n",
    "alcoholism_dataset = pd.concat([alcoholism_2018_df, alcoholism_2019_df, alcoholism_post_df, alcoholism_pre_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from drive\n",
    "depression_2018_df = pd.read_csv(data_path + \"/depression/depression_2018.csv\")\n",
    "depression_2019_df = pd.read_csv(data_path + \"/depression/depression_2019.csv\")\n",
    "depression_post_df = pd.read_csv(data_path + \"/depression/depression_post.csv\")\n",
    "depression_pre_df  = pd.read_csv(data_path + \"/depression/depression_pre.csv\")\n",
    "\n",
    "# join all data into one DataFrame\n",
    "depression_dataset = pd.concat([depression_2018_df, depression_2019_df, depression_post_df, depression_pre_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of post from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of our data\n",
    "print('adhd', len(adhd_dataset))\n",
    "print('anxiety', len(anxiety_dataset))\n",
    "print('alcoholim', len(alcoholism_dataset))\n",
    "print('depression', len(depression_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhd_post = adhd_dataset.loc[:, \"post\"][1]\n",
    "adhd_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data noise reduction and format simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhd_posts = [i for i in adhd_dataset.loc[:, \"post\"]]\n",
    "anxiety_posts = [i for i in anxiety_dataset.loc[:, \"post\"]]\n",
    "alcoholism_posts = [i for i in alcoholism_dataset.loc[:, \"post\"]]\n",
    "depression_posts = [i for i in depression_dataset.loc[:, \"post\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Data Splitting and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to remove stopwords from posts\n",
    "\n",
    "def remove_stops(text, stops):\n",
    "    words = text.split()\n",
    "    final = []\n",
    "    for word in words:\n",
    "        if word not in stops:\n",
    "            final.append(word)\n",
    "    final = \" \".join(final)\n",
    "    final = final.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    final = \"\".join([i for i in final if not i.isdigit()])\n",
    "    while \"  \" in final:\n",
    "        final = final.replace(\"  \", \" \")\n",
    "    return final\n",
    "\n",
    "def clean_docs(docs):\n",
    "    stops = stopwords.words(\"english\")\n",
    "    final = []\n",
    "    final2 = []\n",
    "    for doc in docs:\n",
    "        clean_doc = remove_stops(doc, stops)\n",
    "        final.append(clean_doc)\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_adhd_docs = clean_docs(adhd_posts)\n",
    "cleaned_anxiety_docs = clean_docs(anxiety_posts)\n",
    "cleaned_alcoholism_docs = clean_docs(alcoholism_posts)\n",
    "cleaned_depression_docs = clean_docs(depression_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for an data loss\n",
    "\n",
    "print('adhd', len(cleaned_adhd_docs))\n",
    "print('anxiety', len(cleaned_anxiety_docs))\n",
    "print('alcoholism', len(cleaned_alcoholism_docs))\n",
    "print('depression', len(cleaned_depression_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare clean vs unclean samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ADHD dataset\n",
    "adhd_posts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_adhd_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From alcoholism dataset\n",
    "alcoholism_posts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_alcoholism_docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "We struggled through a lot of difficulties during the process of this project. Although we did not end up using our TF-IDF direct results, we are including them here because they are the basis on which we later built our other very similar text vectorization layer in the next section. Although it was not a direct predecessor to the following steps in the project, it established a useful foundation for the structure of the next vectorization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "#custom_stopwords = 'drive/MyDrive/Courses UCSD/SPRING 2022/COGS 118A/FinalProject/stop_words_english.txt'\n",
    "custom_stopwords = 'stop_words_english.txt'\n",
    "with open(custom_stopwords, 'r') as f:\n",
    "    more_stop_words = [line.strip() for line in f]\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(more_stop_words)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "                                lowercase=True,\n",
    "                                max_features=300,\n",
    "                                stop_words=my_stop_words)\n",
    "\n",
    "vectors = vectorizer.fit_transform(cleaned_adhd_docs)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "adhd_tfidf = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhd_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize only the keywords\n",
    "\n",
    "all_keywords = []\n",
    "\n",
    "for description in denselist:\n",
    "    x = 0\n",
    "    keywords = []\n",
    "    for word in description:\n",
    "        if word > 0:\n",
    "            keywords.append(feature_names[x])\n",
    "        x = x+1\n",
    "    all_keywords.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_keywords[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if we are interested in the n features with the highest TF IDF scores\n",
    "\n",
    "top_n = 300\n",
    "top_n_features = sorted(list(zip(feature_names, \n",
    "                                  vectors.sum(0).getA1())), \n",
    "                              key=lambda x: x[1], reverse=True)[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in top_n_features:\n",
    "    if feature[0] == 'suicide':\n",
    "        print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the TF-IDF seed words from the 2018 depression dataset\n",
    "my_seed_words = []\n",
    "for feature in top_n_features:\n",
    "    my_seed_words.append(feature[0])\n",
    "print(my_seed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF seed words from existing study\n",
    "depression_true_seed_words = ['myself', 'really', 'depression', 'hope', 'life', 'forever', 'pain', 'sad', 'live', 'mood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cosine Similarity between the two seed word lists\n",
    "from collections import Counter\n",
    "\n",
    "# count word occurrences\n",
    "our_vals = Counter(my_seed_words)\n",
    "true_vals = Counter(depression_true_seed_words)\n",
    "\n",
    "# convert to word-vectors\n",
    "words  = list(our_vals.keys() | true_vals.keys())\n",
    "our_vect = [our_vals.get(word, 0) for word in words]        # [0, 0, 1, 1, 2, 1]\n",
    "true_vect = [true_vals.get(word, 0) for word in words]        # [1, 1, 1, 0, 1, 0]\n",
    "\n",
    "# find cosine\n",
    "len_our  = sum(av*av for av in our_vect) ** 0.5             # sqrt(7)\n",
    "len_true  = sum(bv*bv for bv in true_vect) ** 0.5             # sqrt(4)\n",
    "dot    = sum(av*bv for av,bv in zip(our_vect, true_vect))    # 3\n",
    "cosine = dot / (len_our * len_true)                          # 0.5669467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-Like Masked Language Model for Self-Supervised Text Classification\n",
    "\n",
    "The following code creates labels automatically using Masked Language Modeling, with one Neural Network layer for our dataset, since reddit posts unfortunately do not come prelabeled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the adhd features into a tensor\n",
    "adhd_features = tf.constant(cleaned_adhd_docs)\n",
    "\n",
    "# make the adhd labels into a tensor\n",
    "# these are temporary labels just to make a temporary dataset\n",
    "adhd_labels = tf.constant(np.random.choice([0, 1], size=(len(cleaned_adhd_docs),), p=[1./3, 2./3]))\n",
    "\n",
    "# initialize a tensorflow dataset for text features and labels\n",
    "# wel will use this dataset to extract a lexicon out of all data samples\n",
    "# so that we can train a neural network with it\n",
    "adhd_dataset = tf.data.Dataset.from_tensor_slices((adhd_features, adhd_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this just displays the first couple data points and their classification label\n",
    "\n",
    "for text_batch, label_batch in adhd_dataset.take(3):\n",
    "        print(text_batch.numpy())\n",
    "        print(label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Prep: Vocabulary and Mask Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data cleaning from capitalization and symbols\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence lenght.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize vocabulary layer, creates a lexicon to adapt our model with\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "\n",
    "    # use the entire dataset (no labels) and create a useful lexicon out of it:\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    # Insert mask token in vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Vectorization with Vocabulary Layer\n",
    "- the layer will build a vocabulary of all string tokens seen in the dataset, sorted by occurance count, with ties broken by sort order of the tokens (high to low).\n",
    "- Will compute the most frequent tokens occurring in the input dataset.\n",
    "- We use this 'vocab' to train our model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ADHD'''\n",
    "# run our cleaned ADHD data through the vocab layer\n",
    "adhd_vectorize_layer = get_vectorize_layer(\n",
    "    cleaned_adhd_docs,\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Get mask token id for masked language model\n",
    "adhd_mask_token_id = adhd_vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ANXIETY'''\n",
    "# run our cleaned ADHD data through the vocab layer\n",
    "anxiety_vectorize_layer = get_vectorize_layer(\n",
    "    cleaned_anxiety_docs,\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Get mask token id for masked language model\n",
    "anxiety_mask_token_id = anxiety_vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ALCOHOLISM'''\n",
    "# run our cleaned ADHD data through the vocab layer\n",
    "alcoholism_vectorize_layer = get_vectorize_layer(\n",
    "    cleaned_alcoholism_docs,\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Get mask token id for masked language model\n",
    "alcoholism_mask_token_id = alcoholism_vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT ENOUGH RAM ON COLAB\n",
    "\n",
    "'''DEPRESSION'''\n",
    "# run our cleaned ADHD data through the vocab layer\n",
    "depression_vectorize_layer = get_vectorize_layer(\n",
    "    cleaned_depression_docs,\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Get mask token id for masked language model\n",
    "depression_mask_token_id = depression_vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding and Self-Classification with Masked Language Modeling\n",
    "Code sample from Keras Official Documentation: https://keras.io/examples/nlp/masked_language_modeling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the function which creates automatic labels for our dataset\n",
    "by using the vectorization and vocab layer we created previously\n",
    "'''\n",
    "def get_masked_input_and_labels(encoded_texts, mask_token_id):\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Automatic Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ADHD'''\n",
    "# Prepare data for masked language model for the unlabeled ADHD dataset\n",
    "x_all_adhd = adhd_vectorize_layer(cleaned_adhd_docs).numpy()\n",
    "x_masked_adhd_train, y_masked_adhd_labels, adhd_sample_weights = get_masked_input_and_labels(\n",
    "    x_all_adhd, adhd_mask_token_id)\n",
    "\n",
    "# formulate our new self-labeled dataset\n",
    "mlm_adhd_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_adhd_train, y_masked_adhd_labels, adhd_sample_weights))\n",
    "mlm_adhd_ds = mlm_adhd_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Anxiety'''\n",
    "# Prepare data for masked language model for the unlabeled anxiety dataset\n",
    "x_all_anxiety = anxiety_vectorize_layer(cleaned_anxiety_docs).numpy()\n",
    "x_masked_anxiety_train, y_masked_anxiety_labels, anxiety_sample_weights = get_masked_input_and_labels(\n",
    "    x_all_anxiety, anxiety_mask_token_id)\n",
    "\n",
    "# formulate our new self-labeled dataset\n",
    "mlm_anxiety_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_anxiety_train, y_masked_anxiety_labels, anxiety_sample_weights))\n",
    "mlm_anxiety_ds = mlm_anxiety_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Alcoholism'''\n",
    "# Prepare data for masked language model for the unlabeled alcoholism dataset\n",
    "x_all_alcoholism = alcoholism_vectorize_layer(cleaned_alcoholism_docs).numpy()\n",
    "x_masked_alcoholism_train, y_masked_alcoholism_labels, alcoholism_sample_weights = get_masked_input_and_labels(\n",
    "    x_all_alcoholism, alcoholism_mask_token_id)\n",
    "\n",
    "# formulate our new self-labeled dataset\n",
    "mlm_alcoholism_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_alcoholism_train, y_masked_alcoholism_labels, alcoholism_sample_weights))\n",
    "mlm_alcoholism_ds = mlm_alcoholism_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Depression'''\n",
    "# Prepare data for masked language model for the unlabeled depression dataset\n",
    "x_all_depression = depression_vectorize_layer(cleaned_depression_docs).numpy()\n",
    "x_masked_depression_train, y_masked_depression_labels, depression_sample_weights = get_masked_input_and_labels(\n",
    "    x_all_depression, depression_mask_token_id)\n",
    "\n",
    "# formulate our new self-labeled dataset\n",
    "mlm_depression_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_depression_train, y_masked_depression_labels, depression_sample_weights))\n",
    "mlm_depression_ds = mlm_depression_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeled ADHD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of our data\n",
    "print('adhd', len(mlm_adhd_ds))\n",
    "print('anxiety', len(mlm_anxiety_ds))\n",
    "print('alcoholism', len(mlm_alcoholism_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BERT Model (Pretraining Model) for masked language modeling\n",
    "It will take token ids as inputs (including masked tokens) and it will predict the correct ids for the masked input tokens.  \n",
    "Code sample from Keras Official Documentation: https://keras.io/examples/nlp/masked_language_modeling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## please note this bert module is from Keras Documentation\n",
    "## it is included here because tensorflow or keras do not have a set of\n",
    "## functions we can just use for this, we have to include them here\n",
    "\n",
    "def bert_module(query, key, value, i):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}/multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_{}/ffn\".format(i),\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",\n",
    "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "id2token = dict(enumerate(alcoholism_vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}\n",
    "mask_token_id = alcoholism_mask_token_id\n",
    "\n",
    "# anxiety_id2token = dict(enumerate(anxiety_vectorize_layer.get_vocabulary()))\n",
    "# anxiety_token2id = {y: x for x, y in anxiety_id2token.items()}\n",
    "\n",
    "# alcoholism_id2token = dict(enumerate(alcoholism_vectorize_layer.get_vocabulary()))\n",
    "# alcoholism_token2id = {y: x for x, y in alcoholism_id2token.items()}\n",
    "\n",
    "# depression_id2token = dict(enumerate(depression_vectorize_layer.get_vocabulary()))\n",
    "# depression_token2id = {y: x for x, y in depression_id2token.items()}\n",
    "\n",
    "\n",
    "\n",
    "## optional text generator\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this callback can show us the evolution of our training\n",
    "adhd_sample_tokens = adhd_vectorize_layer([\"Lately I have been feeling [mask] and I do not know what to do\"])\n",
    "generator_callback = MaskedTextGenerator(adhd_sample_tokens.numpy())\n",
    "\n",
    "bert_masked_adhd_model = create_masked_language_bert_model()\n",
    "bert_masked_adhd_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert model on a much smaller dataset\n",
    "\n",
    "## this callback can show us the evolution of our training\n",
    "alcoholism_sample_tokens = alcoholism_vectorize_layer([\"I am so happy to be [mask] now. Daily drinking was ruining my life.\"])\n",
    "generator_callback = MaskedTextGenerator(alcoholism_sample_tokens.numpy())\n",
    "\n",
    "bert_masked_alcoholism_model = create_masked_language_bert_model()\n",
    "bert_masked_alcoholism_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately takes like 15 hours\n",
    "\n",
    "#bert_masked_adhd_model.fit(mlm_adhd_ds, epochs=2, callbacks=[generator_callback])\n",
    "#bert_masked_adhd_model.save(data_path + \"/adhd/bert_mlm_adhd.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_masked_alcoholism_model.fit(mlm_alcoholism_ds, epochs=2, callbacks=[generator_callback])\n",
    "# bert_masked_alcoholism_model.save(data_path + \"/alcoholism/bert_mlm_alcoholism.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained bert model\n",
    "alcoholism_mlm_model = keras.models.load_model(\n",
    "    data_path+\"/alcoholism/bert_mlm_alcoholism.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    ")\n",
    "pretrained_bert_model = tf.keras.Model(\n",
    "    alcoholism_mlm_model.input, alcoholism_mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output\n",
    ")\n",
    "\n",
    "# Freeze it\n",
    "pretrained_bert_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "    sequence_output = pretrained_bert_model(inputs)\n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    classifer_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return classifer_model\n",
    "\n",
    "alcoholism_classifer_model = create_classifier_bert_model()\n",
    "alcoholism_classifer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text_batch, label_batch, weights in mlm_adhd_ds.take(1):\n",
    "#     for i in range(1):\n",
    "#         print('text vector\\n', text_batch.numpy()[i])\n",
    "#         print('label\\n', label_batch.numpy()[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model previously split 32% of the data for this auto-classification task\n",
    "\n",
    "DATASET_SIZE = len(list(mlm_adhd_ds))\n",
    "DATASET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this piece of code lets us control the train/test split\n",
    "# it splits the adhd tensorflow dataset and splits it\n",
    "\n",
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "test_size = int(0.3 * DATASET_SIZE)\n",
    "\n",
    "adhd_train_dataset = mlm_adhd_ds.take(train_size)\n",
    "adhd_test_dataset = mlm_adhd_ds.skip(train_size)\n",
    "\n",
    "print(\"train\", len(list(adhd_train_dataset)))\n",
    "print(\"test\", len(list(adhd_test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tensorflow datasets into x and y lists to use for sklearn\n",
    "\n",
    "adhd_train_y = []\n",
    "adhd_train_x = []\n",
    "\n",
    "# training data split into text vectorizations and vectorized labels\n",
    "adhd_train_x = np.array([list(x[0].numpy()) for x in list(adhd_train_dataset)])\n",
    "adhd_train_y = np.array([x[1].numpy() for x in list(adhd_train_dataset)])\n",
    "\n",
    "# test data split into text vectorizations and vectorized labels\n",
    "adhd_test_x = np.array([list(x[0].numpy()) for x in list(adhd_test_dataset)])\n",
    "adhd_test_y = np.array([x[1].numpy() for x in list(adhd_test_dataset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just view the length of our dataset, make sure its the right number of training samples\n",
    "\n",
    "len(adhd_train_x), len(adhd_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM\n",
    "# we have a dataset with a very weird shape, making it difficult to put into\n",
    "# any sklearn fit() function. We need to reduce the dimensionality of the data\n",
    "# somehow to be able to train with it\n",
    "\n",
    "np.array(adhd_train_x).shape, np.array(adhd_train_y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = len(list(mlm_alcoholism_ds))\n",
    "print(\"data size \", DATASET_SIZE)\n",
    "\n",
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "test_size = int(0.3 * DATASET_SIZE)\n",
    "\n",
    "alcoholism_train_dataset = mlm_alcoholism_ds.take(train_size)\n",
    "alcoholism_test_dataset = mlm_alcoholism_ds.skip(train_size)\n",
    "\n",
    "print(\"train\", len(list(alcoholism_train_dataset)))\n",
    "print(\"test\", len(list(alcoholism_test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier with frozen BERT stage\n",
    "alcoholism_classifer_model.fit(\n",
    "    alcoholism_train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=alcoholism_test_dataset,\n",
    ")\n",
    "\n",
    "# Unfreeze the BERT model for fine-tuning\n",
    "pretrained_bert_model.trainable = True\n",
    "optimizer = keras.optimizers.Adam()\n",
    "alcoholism_classifer_model.compile(\n",
    "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "alcoholism_classifer_model.fit(\n",
    "    alcoholism_train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=alcoholism_test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(adhd_train_x, adhd_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "\n",
    "clf.fit(adhd_train_x, adhd_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lin_svm = OneVsRestClassifier(LinearSVC(random_state=0)).fit(adhd_train_x, adhd_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "Unfortunately, our model ran into too many issues and we were not able to produce any concrete results in terms of the evaluation metrics we wanted to analyze. Overall, the selection of our dataset proved to be one of main issues that we ran into when trying to implement our models. Due to the unlabelled nature of our dataset, we either had to hand label thousands of rows of data, or find some other solution to use the data in our supervised learning models. Initially, we planned on using tf-idf to generate labels from the text we had already collected, and to use lexicons other researchers had developed to label our data. However, we could not move out of this stage due to the private nature of many lexicons. After this, we tried using deep learning to generate labels using a BERT Transformer-Encoder network architecture which automatically labels the datasets. This proved to be mostly successful, as we were able to label our datasets. However, when we reached the model training and testing phase, we found that the output of the BERT Transformer-Encoder network architecture was unusable for our planned KNN and SVM models. We were unable to solve this bug and unfortunately ran out of time to complete our implementation. \n",
    "\n",
    "### Limitations\n",
    "\n",
    "Overall, our project had many, many limitations and shortcomings. With more time, we would have liked to either find a different, labeled, dataset, or work more with our neural network auto labeling code to output labels that are usable by the algorithms we wanted to test out. Our approach shows quite a bit of promise, and we believe that this research is something that is worth investigating further with more time. With our initial attempts, we did indeed find ways to autolabel our datasets, pointing to potential research that could be done with deep neural networks trained on internet text posts to discover mental health issues. Furthermore, we were limited by the amount of RAM we had to use, and found that we were often out of RAM to properly generate labels and train our neural networks. With better hardware, we could expand on our project to generate the results we were hoping for.   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Because our project and data potentially involves sensitive or intimate information regarding a person’s mental health, there are clear ethical and privacy concerns. To preserve anonymity, we plan on removing or assigning anonymous ID to the authors’ usernames from each observation during our data cleaning process.\n",
    "Additionally, extra processing to the text can be done to remove sensitive information such as names, addresses, etc. It is also important to note that the data was originally collected using Reddit’s API, which pulls from publicly available subreddits. This means we are not in violation of any major privacy sectors and all of the data we use can be found on the open web.\n",
    "In terms of medical ethics, we hope our model can be used as simply a preventative aid and optional resource to Reddit users who may require mental health diagnosis. Most importantly, to never carry more weight than the opinion of a medical professional.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Overall, we wanted to create a model that could correctly identify mental health disorders through text posts found on the internet. We wanted this to be a tool that could support people struggling with mental health issues by pointing them in the right direction to seek professional medical help. However, we faced numerous challenges throughout the implementation of our project and unfortunately could not bring it to fruition. With more time, we would want to find a better labeled dataset to use for supervised learning, as well as utilize deep learning techniques to see if we can better detect mental health disorders. Once achieved, this project would fit with other work in this field [2, 3] to boost mental health detection and prevention of its aggravation for a diverse population. In the future, the practicality of this experiment would extend into the implementation of notification alerts to reddit users privately, if their posts are detected to have a potential for self-harm or a diagnosis of a mental disorder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "[1] Alegria, Margarita, Jackson, James S. (James Sidney), Kessler, Ronald C., and Takeuchi, David. Collaborative Psychiatric Epidemiology Surveys (CPES), 2001-2003 [United States]. Inter-university Consortium for Political and Social Research [distributor], 2016-03-23. https://doi.org/10.3886/ICPSR20240.v8  \n",
    "[2] Prerna Chikersal, Afsaneh Doryab, Michael Tumminia, Daniella K. Villalba, Janine M. Dutcher, Xinwen Liu, Sheldon Cohen, Kasey G. Creswell, Jennifer Mankoff, J. David Creswell, Mayank Goel, Anind Dey. 2020. Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing: A Machine Learning Approach With Robust Feature Selection. ACM Transactions on Computer-Human Interaction (TOCHI), 2020.  \n",
    "[3] Low, D. M., Rumker, L., Torous, J., Cecchi, G., Ghosh, S. S., & Talkar, T. (2020). Natural Language Processing Reveals Vulnerable Mental Health Support Groups and Heightened Health Anxiety on Reddit During COVID-19: Observational Study. Journal of medical Internet research, 22(10), e22635. https://zenodo.org/record/3941387#.YmXlUNPMKDU  \n",
    "[4] Kuwamura, Kaiko, et al. \"Hugvie: A medium that fosters love.\" 2013 IEEE RO-MAN. IEEE, 2013.  \n",
    "[5] Šabanović, Selma, et al. \"PARO robot affects diverse interaction modalities in group sensory therapy for older adults with dementia.\" 2013 IEEE 13th international conference on rehabilitation robotics (ICORR). IEEE, 2013.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
